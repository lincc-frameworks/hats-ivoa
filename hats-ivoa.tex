\documentclass[11pt,a4paper]{ivoa}

\usepackage{xcolor}  % for colored text

\input tthdefs
\input gitmeta

\title{HATS: A Standard for the Hierarchical Adaptive Tiling Scheme in the Virtual Observatory}

\ivoagroup{Applications Group}

%\author[https://www.ivoa.net/authors/caplar]{Neven Caplar}
%\author[https://www.ivoa.net/authors/usher]{Melissa DeLucchi }
%\author[https://www.ivoa.net/authors/offline]{Mario Juric}
\author[https://www.ivoa.net/authors/offline]{LINCC team...}

\editor{editor here}

\previousversion{This is the first public release}

\begin{document}
\begin{abstract}
    The increasing complexity and volume of astronomical datasets necessitate efficient spatial indexing and query strategies within the Virtual Observatory (VO). The Hierarchical Adaptive Tiling Scheme (HATS) is a framework designed to facilitate scalable queries, filtering operations, and efficient data retrieval across large astronomical surveys. Traditional spatial indexing methods often struggle with the massive scale of modern astronomical datasets, leading to inefficient query execution and storage overhead. HATS provides a flexible, hierarchical approach that balances computational efficiency and adaptability to non-uniform data distributions.

    This document describes the structure, implementation, and best practices for integrating HATS within the VO ecosystem, ensuring interoperability and performance optimization for distributed astronomical datasets. The reference implementation of HATS can be found at \url{https://github.com/astronomy-commons/hats}. Additionally, we discuss how HATS enhances existing indexing schemes, its role in federated data access, and its potential applications for time-domain astronomy, large-scale surveys, and cross-matching of astronomical catalogs.
\end{abstract}

\section*{Acknowledgments}
    The authors thank the IVOA Applications Working Group and various contributors from the astronomical community for their feedback and discussions that shaped this standard. We acknowledge the key collaborations with Space Telescope Science Institute (STScI) and IPAC, whose expertise and contributions have been invaluable in refining the HATS framework. Additionally, we extend our gratitude to  the Strasbourg astronomical Data Center for their assistance in metadata structuring and interoperability support. This work has also benefited from insights provided by the S-Plus survey, Linea, European Space Agency, and Canadian astronomy data center, whose contributions have helped enhance the applicability and robustness of HATS in large-scale astronomical data analysis.


\section*{Conformance-related definitions}
The words ``MUST'', ``SHALL'', ``SHOULD'', ``MAY'', ``RECOMMENDED'', and
``OPTIONAL'' (in upper or lower case) used in this document are to be
interpreted as described in IETF standard RFC2119 \citep{std:RFC2119}.

The \emph{Virtual Observatory (VO)} is a
general term for a collection of federated resources that can be used
to conduct astronomical research, education, and outreach.
The \href{https://www.ivoa.net}{International
Virtual Observatory Alliance (IVOA)} is a global
collaboration of separately funded projects to develop standards and
infrastructure that enable VO applications.

\section{Introduction}
    The rapid expansion of astronomical data from large survey facilities like Vera C. Rubin Observatory, Euclid, and Roman Space Telescope necessitates innovative solutions for spatial indexing and efficient data retrieval. These surveys generate vast amounts of high-resolution imaging and time-domain data, requiring efficient methods for organizing, querying, and cross-matching data across multiple archives. Traditional approaches to spatial indexing, such as hierarchical pixelization (e.g., HEALPix) or static tiling schemes, often exhibit inefficiencies when handling dynamic, multi-resolution datasets.

    The Hierarchical Adaptive Tiling Scheme (HATS) is a novel approach designed to optimize spatial data partitioning while maintaining flexibility in accommodating varying data densities. Unlike fixed spatial partitioning methods, HATS dynamically adjusts tile sizes based on local data characteristics, ensuring an optimal balance between resolution, query efficiency, and storage management. By leveraging a hierarchical structure, HATS allows users to perform efficient multi-resolution queries while preserving high precision in regions of interest.

    The HATS standard aims to define best practices for implementing and utilizing this tiling scheme within the Virtual Observatory framework. This document outlines the principles behind HATS, describes its data model, and provides recommendations for integrating it into existing VO services. Additionally, we discuss how HATS can facilitate efficient cross-matching of astronomical catalogs, accelerate large-scale spatial queries, and enhance interoperability between diverse astronomical datasets.


\section{Motivation and Goals}
    The primary motivation behind HATS is to address the following challenges in astronomical data management:
    \begin{itemize}
        \item \textbf{Scalability:} Modern astronomical surveys generate petabytes of spatially distributed data, requiring an indexing scheme that scales efficiently with dataset size.
        \item \textbf{Adaptive Resolution:} Fixed grid-based partitioning often leads to inefficient storage and query execution, particularly in non-uniformly distributed datasets. HATS dynamically adjusts tile sizes to accommodate varying data densities.
        \item \textbf{Efficient Query Execution:} Spatial queries such as nearest-neighbor searches and cross-matching must be executed efficiently across distributed data repositories. HATS enables rapid indexing and retrieval of relevant data subsets.
        \item \textbf{Interoperability:} Astronomical data is collected from diverse instruments and observatories, often using different spatial reference frames. HATS provides a standardized framework for integrating and harmonizing spatial data across multiple sources.
    \end{itemize}

\section{HATS Design and Implementation}
    The HATS framework consists of the following core components and concepts:
    
\subsection{Catalog collections}
    
At the top level, we separate multiple possible `catalogs` into their respective folder. In this context `catalog` refers to the data storage object which will be described in sections below (starting with Section~\ref{sec:catalog}). The main data is stored in the `catalog` folder while all other `catalogs` are optional, and can be used to enhance access to the main data `catalog` or to enhance the main data `catalog` with additional information. We show the outline of this structure below, with few common examples of these optional `catalogs`:
    
\begin{verbatim}
* gaia_dr3/
    * hats.properties
    * catalog/
        * properties
        * ...
    *  [Optional] margin_5arcs/
        * properties
        * ...
    * [Optional]  index_designation/
        * properties
        * ...
    *  [Optional] spectra/
        * ...    
\end{verbatim}    
    
\textcolor{red}{TODO, NEED HELP -  Explain the structure of the hats.properties. Explain what the format is, what is contained, what is the obligatory and what is optional information}

\subsection{Structure of a catalog} \label{sec:catalog}
\textcolor{red}{TODO, NEED HELP - is the optional designation correct for each one of these metdata files} Focusing now on an individual `catalog`, the catalog organization structure is shown below
\begin{verbatim}
* catalog/
    * properties
    * [Optional] partition_info.csv
    * [Optional] point_map.json
    * [Optional] data_thumbnail.parquet
    * dataset/
        * metadata.json
        * [Optional] common_metadata.json
        * Norder=0/
        * Norder=1/
        * Norder=2/
        * Norder=3/
        * Norder=4/
        * Norder=5/
        * Norder=6/
        * Norder=.../
\end{verbatim}

The astronomy data is stored in the directory dataset, withing the subdirectories that specify the order at which particular part of the dataset is stored. We will discuss the the partitioning and data storage in Sections \ref{sec:hierarchical}, \ref{sec:adaptive} and \ref{sec:parquet}. The other files visible above are various metadata and auxiliary files that are here to enable better and easier handling of the data and will be described in Section \ref{sec:meta}. 
    
\subsection{Hierarchical Structure} \label{sec:hierarchical}
    Focusing now on the contents of the dataset, HATS employs a multi-level hierarchy based on healpix tiling where each level represents a progressively finer spatial resolution.
For instance, if we look into any single level we see the following structure 
\begin{verbatim}
* Norder=x/
    * Dir=0/
    * Npix=yyy/
        * part0.parquet
        * ...
    * Npix=yyy/
        * part0.parquet
        * ...
    * Dir=w/
        * ...
\end{verbatim} 

In the directory structure above, `x`, `yyy`, `zzz`, `w` are understood to be placeholders for positive integer numbers. \textcolor{red}{TODO help needed, How are these numbers determined, that specify Npix and Directory; what is the meaning of the directory number?}. \par
The data is stored in the parquet files (discussed in \ref{sec:parquet}), with one or multiple files being possible in the final directory, i.e., in the ultimate data leaf. If there are multiple files, they should be read together, i.e., we consider them to be one single data unit. In this way small updates can be added to already existing catalogs with simple, correctly placed, addition of files in existing folders.

    \subsection{Adaptive Tiling Algorithm} \label{sec:adaptive}
    Unlike static partitioning schemes, HATS dynamically subdivides spatial regions based on data density. In regions with sparse data, larger tiles minimize storage overhead, whereas high-density regions are subdivided into smaller tiles to improve query efficiency. \par
    The data is stored at a given level, until the dataset size crosses a predetermined threshold. This threshold can be, most commonly,  number of rows, or size of the data on the disk. At this point the data get split into 4 higher order healpix tiles, using the spatial information contained in the data. This process continues until all of the data is stored at the appropriate level, and no ``data leaf`` has more data than the predetermined threshold.

     \subsection{Structure of files} \label{sec:parquet}
The astronomical data is stored in *.parquet format. Parquet is a columnar storage file format optimized for efficient data compression and retrieval, especially well-suited for analytical workloads. It is ideal for storing large amounts of astronomical tabular data because it allows fast access to specific columns without reading the entire dataset, significantly reducing I/O and improving performance. \par
The HATS format requires that the index colum of the dataset is \texttt{hats\_29} index column.  \texttt{hats\_29} stores the crucial spatial information about the position on the sky for each row, and it is not unique.\textcolor{red}{TODO, NEED HELP please describe how it is calculated; explain how it is not unique when multiple rows at the same healpix of order 29 position} 

    \subsection{Metadata and Auxiliary Files} \label{sec:meta}
    HATS implementations utilize auxiliary files and metadata files to store relevant information about the  structure, including:
    \begin{itemize}
    	\item \textbf{hats.properties}, at catalog collection level
         \item \textbf{[Optional] partition\_info.csv:}, at catalog level
        \item \textbf{[Optional] point\_map.json:}, at catalog level
        \item \textbf{[Optional] data\_thumbnail\.parquet}, at catalog level
        \item \textbf{properties:}, at catalog level
        \item \textbf{metadata\.json:}, at catalog/dataset level
        \item \textbf{[Optional] common\_metadata.json:}, at catalog/dataset level
    \end{itemize}
    
    \subsubsection{hats.properties} 
    \textcolor{red}{TODO, NEED HELP - 1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information} 
        \subsubsection{partition\_info.csv} 
   \textcolor{red}{TODO, NEED HELP-1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information}
	Norder, Npix  information here. 
    
        \subsubsection{point\_map.json} 
   \textcolor{red}{TODO, NEED HELP -1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information}
   
           \subsubsection{data\_thumbnail.parquet} 
  This is small dataset aimed to help users to understand and use the data. It is created from taking a first row from each each data leaf, so the the number of rows in this parquet file is the same as the number of  data leafs all together. \par
  This files allows an user to get a quick overview of the whole dataset in a same format as the whole dataset. Given how it is sampled it will be cover the whole width of the dataset and give an user an accurate overview of the properties of the dataset. In such way it is superior and more convenient than pointing an user to taking out a subset of a single parquet data leaf for testing. 
    
        \subsubsection{properties} 
   \textcolor{red}{TODO, NEED HELP-1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information}
     total rows, columns statistics (min, max) , java properties file , ra, dec, default columns, how it was created 
    
    
        \subsubsection{metadata.json} 
   \textcolor{red}{TODO, NEED HELP -1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information}
    It contains per partition information
        \subsubsection{common\_metadata.json} 
   \textcolor{red}{TODO, NEED HELP -1. what does it do 2. what is the format 4. what information does it contain, 4. what is the obligatory part of that information}n
    Information that is true for everything 

    \subsection{Integration with Existing VO Standards}
    HATS is designed to be compatible with existing VO spatial indexing frameworks, such as HEALPix and MOC (Multi-Order Coverage maps). \par
   \textcolor{red}{TODO, NEED HELP, especially with MOC: explain more how is it compatible}. \par 
   \textcolor{red}{ TODO how it will work with TAP, VOParquet}

    \subsection{Performance Considerations}
   \textcolor{red}{TODO parquet enables columns. Rowgroups. Server-side filtering. Larger than memory datasets Benchmarking plots}

\appendix
\section{Changes from Previous Versions}
No previous versions yet.

\bibliography{ivoatex/ivoabib,ivoatex/docrepo}

\end{document}

